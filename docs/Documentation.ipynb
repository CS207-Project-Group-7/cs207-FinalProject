{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "XqirvYUFggX9",
        "BozZIls9KZRv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "XqirvYUFggX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "The goal of this project is to develop a library for performing automatic differentiation (AD), which we will call ```LazyDiff```.\n",
        "AD is a set of techniques to numerically evaluate the derivative of a function motivated by deficiencies in classical methods. Numerical approximation is often not accurate enough and can be computationally expensive and symbolic differentiation tends to lead to inefficient code and faces the difficulty of converting a computer program into a single expression. Both classical methods have problems with calculating higher derivatives, where the complexity and errors increase, and are slow when it comes to computing the partial derivatives of a function with respect to many inputs. Automatic differentiation endeavors to solve all of these problems, and has been used in neural networks for back propagation and weight adjustment based on the loss function, and in scientific computing where it is often difficult to analytically compute the derivatives.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6-b0Gl2LrkBT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# How to Use LazyDiff\n",
        "\n",
        "## How to Install\n",
        "\n",
        "If you want to install the package only for the current virtual environment, run the following commands to create and activate a new virtual env:\n",
        "\n",
        "- Move to the desired working directory\n",
        "- `virtualenv env`\n",
        "- `source env/bin/activate`\n",
        "\n",
        "If you are cloning our repo and installing lazydiff that way, you can run the following to install the dependencies:\n",
        "- `pip install -r requirements.txt`\n",
        "\n",
        "Otherwise, you can execute following command to install LazyDiff and its dependencies:\n",
        " - `pip install git+https://github.com/CS207-Project-Group-7/cs207-FinalProject.git@code`"
      ]
    },
    {
      "metadata": {
        "id": "Ye-aZbj7JTem",
        "colab_type": "code",
        "outputId": "db54dc2e-2d10-41b5-cda3-53e88a108b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "# REMEMBER TO ACTIVATE YOUR VIRTUALENV FIRST\n",
        "# Install lazydiff for demos\n",
        "!pip install git+https://github.com/CS207-Project-Group-7/cs207-FinalProject.git@code"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/CS207-Project-Group-7/cs207-FinalProject.git@code\n",
            "  Cloning https://github.com/CS207-Project-Group-7/cs207-FinalProject.git (to revision code) to /tmp/pip-req-build-p2clugye\n",
            "Branch 'code' set up to track remote branch 'code' from 'origin'.\n",
            "Switched to a new branch 'code'\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lazydiff==0.1) (1.14.6)\n",
            "Building wheels for collected packages: lazydiff\n",
            "  Running setup.py bdist_wheel for lazydiff ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-imalahe5/wheels/38/b0/a1/23fd2707fc03de253c749cba036aab7b1e2cf887a06df4abf4\n",
            "Successfully built lazydiff\n",
            "Installing collected packages: lazydiff\n",
            "Successfully installed lazydiff-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CGcNOcxkJPD6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basic Demo\n",
        " \n",
        " ### Variable Objects and Evaluating Functions \n",
        " \n",
        " The values of variables are stored inside `vars` objects as `Var`, which is used as function inputs. Custom functions from `ops` module need to be used to evaluate the functions, which support all the standard math functions from math and numpy libraries. \n",
        " \n",
        " Function values can be evaluated by simply wrapping the variables with the corresponding functions and accessing the values using `.val`\n",
        " \n",
        "Below we show the examples for basic operations and some of our elementary functions. \n",
        "\n",
        "We evaluate $f(x) = x^2$ and $f(y) = -y$ for variables $x=4$ and $y=2$. We also show how we can compound the functions by evaluating $f(z) = 5*z$ where $z = f(x) + f(y)$, $z = f(x) - f(y)$ and $z = \\frac{f(x)}{f(y)}$, respectively.\n",
        " \n",
        " Also, we evaluate $f(a) = sin(a)$ for $a = 0$,  $f(b) = arcsin(b)$ for $b=0$, and $f(c) = \\frac{1}{1 + e^{-{c}}} + log_2(c)+ \\sqrt{c}$  for $c = 1$."
      ]
    },
    {
      "metadata": {
        "id": "vCt7eKNDvsqv",
        "colab_type": "code",
        "outputId": "14e5ead4-b340-45cc-e92a-e51833eaf9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "from lazydiff.vars import Var\n",
        "from lazydiff import ops\n",
        "import math\n",
        "\n",
        "#part 1\n",
        "# creating variable x = 4 and y = 2\n",
        "x = Var(4)\n",
        "y = Var(2)\n",
        "\n",
        "# constructing and evaluating f(x) = x^2 and f(y) = sin(y)\n",
        "fx = x**2\n",
        "fy = -y\n",
        "# compound function f(z)_1 = 5*z where z = fx + fy\n",
        "fz_1 = 5*(fx + fy)\n",
        "# compound function f(z)_2 = 5*z where z = fx - fy\n",
        "fz_2 = 5*(fx - fy)\n",
        "# compound function f(z)_3 = 5*z where z = fx / fy\n",
        "fz_3 = 5*(fx / fy)\n",
        "\n",
        "# accessing the evaluated values\n",
        "# output should print out 16, -2, 70, 90 and -40\n",
        "print(\"Evaluating f(x) = x^2 where x = 4 : {}\".format(fx.val))\n",
        "print(\"Evaluating f(y) = -y where y = 2 : {}\".format(fy.val))\n",
        "print(\"Evaluating f(z) = 5*z where z = f(x) + f(y) : {}\".format(fz_1.val))\n",
        "print(\"Evaluating f(z) = 5*z where z = f(x) - f(y) : {}\".format(fz_2.val))\n",
        "print(\"Evaluating f(z) = 5*z where z = f(x) / f(y) : {}\".format(fz_3.val))\n",
        "\n",
        "#part 2\n",
        "# creating variable a = 0, b = 0 and c = 1\n",
        "a = Var(0)\n",
        "b = Var(0)\n",
        "c = Var(1)\n",
        "\n",
        "# constructing and evaluating f(a) = sin(a), f(b) = arcsin(b) and f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2)\n",
        "fa = ops.sin(a)\n",
        "fb = ops.arcsin(b)\n",
        "fc = ops.logistic(c) + ops.log(c, 2) + ops.sqrt(c)\n",
        "# accessing the evaluated values\n",
        "# output should print out 0, 0 and 1.73\n",
        "print(\"Evaluating f(a) = sin(a) where a = 0 : {}\".format(fa.val))\n",
        "print(\"Evaluating f(b) = arcsin(b) where b = 0 : {}\".format(fb.val))\n",
        "print(\"Evaluating f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) where c = 1 : {}\".format(fc.val))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating f(x) = x^2 where x = 4 : 16.0\n",
            "Evaluating f(y) = -y where y = 2 : -2.0\n",
            "Evaluating f(z) = 5*z where z = f(x) + f(y) : 70.0\n",
            "Evaluating f(z) = 5*z where z = f(x) - f(y) : 90.0\n",
            "Evaluating f(z) = 5*z where z = f(x) / f(y) : -40.0\n",
            "Evaluating f(a) = sin(a) where a = 0 : 0.0\n",
            "Evaluating f(b) = arcsin(b) where b = 0 : 0.0\n",
            "Evaluating f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) where c = 1 : 1.7310585786300048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V_JlykEZyfhr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluating Function Gradients\n",
        "\n",
        "#### Forward Mode\n",
        "\n",
        "As each function is evaluated, the corresponding gradient is also stored in the new output variable. Therefore, each of the output variable contains the gradients from all the previously evaluated functions. This allows us to access the gradient with respect to any of the variables previously computed. For instance, for $f(z)$ from the above example we can compute the gradient with respect to $z$ or $x$ depending on what we pass as input to the gradient method. This also makes it necessary to store the variables of interest somewhere in the program and then use the variable for which we want to evaluate the gradient of as input to the gradient method of the function. \n",
        "\n",
        "Below we show an example of how to access gradients of $f(x)$, $f(y)$, $f(z)$ and $f(a)$, $f(b)$, $f(c)$using `.grad(variable)` method via forward mode."
      ]
    },
    {
      "metadata": {
        "id": "BAFV93rRBOQH",
        "colab_type": "code",
        "outputId": "883aaf5c-2d5b-4794-ed85-0c75ab5a8a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "# accessing gradients\n",
        "#part 1\n",
        "# gradient of f(x) = x^2 w.r.t x is 8\n",
        "x.forward()\n",
        "print(\"Gradient of f(x) = x^2 w.r.t x is {}\".format(fx.grad(x)))\n",
        "# gradient of f(y) = -y w.r.t y is -1\n",
        "y.forward()\n",
        "print(\"Gradient of f(y) = -y w.r.t y is {}\".format(fy.grad(y)))\n",
        "\n",
        "# gradient of fz_1 w.r.t x is d5(x^2-y)/dx = 10x = 40\n",
        "print(\"Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t x is {}\".format(fz_1.grad(x)))\n",
        "# gradient of fz_2 w.r.t x is d5(x^2+y)/dx = 10x = 40\n",
        "print(\"Gradients of f(z) = 5*z where z = f(x) - f(y) w.r.t x is {}\".format(fz_2.grad(x)))\n",
        "# gradient of fz_3 w.r.t x is d5(x^2/(-y))/dx = -5x = -20\n",
        "print(\"Gradients of f(z) = 5*z where z = f(x) / f(y) w.r.t x is {}\".format(fz_3.grad(x)))\n",
        "\n",
        "# illustratation of taking two variables as input \n",
        "# gradient of fz_1 w.r.t y is d5(x^2-y)/dy = -5\n",
        "print(\"Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t y is {}\".format(fz_1.grad(y)))\n",
        "\n",
        "#part 2\n",
        "# gradient of f(a) = sin(a) w.r.t a is 1\n",
        "a.forward()\n",
        "print(\"Gradient of f(a) = sin(a) w.r.t a is {}\".format(fa.grad(a)))\n",
        "# gradient of f(b) = arcsin(b) w.r.t b is 1\n",
        "b.forward()\n",
        "print(\"Gradient of f(b) = arcsin(b) w.r.t y is {}\".format(fb.grad(b)))\n",
        "# gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is 2.14\n",
        "c.forward()\n",
        "print(\"Gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is {}\".format(fc.grad(c)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient of f(x) = x^2 w.r.t x is 8.0\n",
            "Gradient of f(y) = -y w.r.t y is -1.0\n",
            "Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t x is 40.0\n",
            "Gradients of f(z) = 5*z where z = f(x) - f(y) w.r.t x is 40.0\n",
            "Gradients of f(z) = 5*z where z = f(x) / f(y) w.r.t x is -20.0\n",
            "Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t y is -5.0\n",
            "Gradient of f(a) = sin(a) w.r.t a is 1.0\n",
            "Gradient of f(b) = arcsin(b) w.r.t y is 1.0\n",
            "Gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is 2.139306974130445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1L_5GtALDJhK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Reverse Mode\n",
        "\n",
        "Below we show an example of how to access gradients of $f(x)$, $f(y)$, $f(z)$ and $f(a)$, $f(b)$, $f(c)$using `.grad(variable)` method via reverse mode."
      ]
    },
    {
      "metadata": {
        "id": "SWtLbtC_DJAJ",
        "colab_type": "code",
        "outputId": "93a6216b-595c-4241-967b-2b17a4d089de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "# accessing gradients\n",
        "#part 1\n",
        "# gradient of f(x) = x^2 w.r.t x is 8\n",
        "fx.backward()\n",
        "print(\"Gradient of f(x) = x^2 w.r.t x is {}\".format(fx.grad(x)))\n",
        "# gradient of f(y) = -y w.r.t y is -1\n",
        "fy.backward()\n",
        "print(\"Gradient of f(y) = -y w.r.t y is {}\".format(fy.grad(y)))\n",
        "\n",
        "# gradient of fz_1 w.r.t x is d5(x^2-y)/dx = 10x = 40\n",
        "fz_1.backward()\n",
        "print(\"Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t x is {}\".format(fz_1.grad(x)))\n",
        "# gradient of fz_2 w.r.t x is d5(x^2+y)/dx = 10x = 40\n",
        "fz_2.backward()\n",
        "print(\"Gradients of f(z) = 5*z where z = f(x) - f(y) w.r.t x is {}\".format(fz_2.grad(x)))\n",
        "# gradient of fz_3 w.r.t x is d5(x^2/(-y))/dx = -5x = -20\n",
        "fz_3.backward()\n",
        "print(\"Gradients of f(z) = 5*z where z = f(x) / f(y) w.r.t x is {}\".format(fz_3.grad(x)))\n",
        "\n",
        "# illustratation of taking two variables as input \n",
        "# gradient of fz_1 w.r.t y is d5(x^2-y)/dy = -5\n",
        "print(\"Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t y is {}\".format(fz_1.grad(y)))\n",
        "\n",
        "#part 2\n",
        "# gradient of f(a) = sin(a) w.r.t a is 1\n",
        "fa.backward()\n",
        "print(\"Gradient of f(a) = sin(a) w.r.t a is {}\".format(fa.grad(a)))\n",
        "# gradient of f(b) = arcsin(b) w.r.t b is 1\n",
        "fb.backward()\n",
        "print(\"Gradient of f(b) = arcsin(b) w.r.t y is {}\".format(fb.grad(b)))\n",
        "# gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is 2.14\n",
        "fc.backward()\n",
        "print(\"Gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is {}\".format(fc.grad(c)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient of f(x) = x^2 w.r.t x is 8.0\n",
            "Gradient of f(y) = -y w.r.t y is -1.0\n",
            "Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t x is 40.0\n",
            "Gradients of f(z) = 5*z where z = f(x) - f(y) w.r.t x is 40.0\n",
            "Gradients of f(z) = 5*z where z = f(x) / f(y) w.r.t x is -20.0\n",
            "Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t y is -5.0\n",
            "Gradient of f(a) = sin(a) w.r.t a is 1.0\n",
            "Gradient of f(b) = arcsin(b) w.r.t y is 1.0\n",
            "Gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is 2.139306974130445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nFRWl-Dl3X3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Working with Vectors\n",
        "\n",
        "Our class Var is a wrapper for numpy arrays that can handle both scalars and vectors.\n",
        "\n",
        "Below we show an example for evaluating the value and the gradients/Jacobian of $f(z) = 2*z$ where $z = f(x,y)$ and  $f(x,y) = x^2+y$ where $x$,$y$ are the vectors:"
      ]
    },
    {
      "metadata": {
        "id": "bxegV6eX-BOh",
        "colab_type": "code",
        "outputId": "5a02586d-8554-4ab8-cce1-3b878c587cde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "# creating vector variables x = (1, 2, 3, 4, 5) y = (1, 2, 3, 4, 5)\n",
        "x = Var([1, 2, 3, 4, 5])\n",
        "y = Var([1, 2, 3, 4, 5])\n",
        "\n",
        "# constructing and evaluating f(x,y) and f(z)\n",
        "fxy = x**2 + y\n",
        "fz = 2*fxy\n",
        "\n",
        "# accessing the evaluated values\n",
        "print(\"Evaluating f(x,y) = x^2 + y : {}\".format(fxy.val))\n",
        "print(\"Evaluating f(z) = 2*z : {}\".format(fz.val))\n",
        "\n",
        "# Jacobian of f(x,y) w.r.t x\n",
        "x.forward()\n",
        "print(\"Jacobian of f(x,y) w.r.t. x:\\n{}\".format(fxy.grad(x)))\n",
        "# Jacobian of f(z) w.r.t x\n",
        "print(\"Jacobian of f(z) w.r.t. x:\\n{}\".format(fz.grad(x)))\n",
        "# Jacobian of f(z) w.r.t y\n",
        "y.forward()\n",
        "print(\"Jacobian of f(z) w.r.t. y:\\n{}\".format(fz.grad(y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating f(x,y) = x^2 + y : [ 2.  6. 12. 20. 30.]\n",
            "Evaluating f(z) = 2*z : [ 4. 12. 24. 40. 60.]\n",
            "Jacobian of f(x,y) w.r.t. x:\n",
            "[ 2.  4.  6.  8. 10.]\n",
            "Jacobian of f(z) w.r.t. x:\n",
            "[ 4.  8. 12. 16. 20.]\n",
            "Jacobian of f(z) w.r.t. y:\n",
            "[2. 2. 2. 2. 2.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OL45Pg0d7shj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Easy-To-Use Newton's Method with lazydiff\n",
        "\n",
        "We show how we can use Newton's Method to solve for root of a question using lazzydiff\n",
        "\n",
        "In this case, we explore this using $f(x) = x^2 - 2$, which has a root at $x=\\sqrt2$"
      ]
    },
    {
      "metadata": {
        "id": "P8_gX--H8Ere",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Newton_Method(f, x0, maxiter=1000):\n",
        "    \"\"\" returns the root for the given function\n",
        "        using Newtons Method\n",
        "        \n",
        "        this function supports only Scalar\n",
        "        you can modify this function to support Vector\n",
        "        \n",
        "        ===INPUT===\n",
        "        f: function written in ops or built_in arithmetic functions\n",
        "        x0: initial point\n",
        "        maxiter: maximum number of iterations\n",
        "    \"\"\"\n",
        "    x = x0\n",
        "    for _ in range(maxiter):\n",
        "        if abs(f(x)).val < 1e-8:\n",
        "            return x\n",
        "        fx = f(x)\n",
        "        fx.backward()\n",
        "        newx = x - fx/fx.grad(x)\n",
        "        if (abs(newx-x).val < 1e-8):\n",
        "            return newx\n",
        "        x = newx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q3_pFeX68jOU",
        "colab_type": "code",
        "outputId": "bac66e9f-17d8-409b-c73d-e7b0add23088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# the function x^2-2 for which we want to find the root of\n",
        "fx = lambda x: x**2-2\n",
        "ans = Newton_Method(fx, Var(1))\n",
        "print(\"The root found by Newton's Method using autodiff is: {}\".format(ans.val))\n",
        "print(\"The difference between this root and the real root 2**0.5 is : {}\".format(ans.val-2**0.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The root found by Newton's Method using autodiff is: 1.4142135623746899\n",
            "The difference between this root and the real root 2**0.5 is : 1.5947243525715749e-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rvK8XHql_9Q7"
      },
      "cell_type": "markdown",
      "source": [
        "# Background\n",
        " AD exploits the fact that every function, no matter how complicated, is a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (such as $\\sin$, $\\cos$, $\\exp$, $\\log$, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
        " * **Chain Rule**  \n",
        "   - AD relies heavily on the chain rule from calculus, which says that $$\\frac{d}{dx} f(g(x)) = \\frac{df}{dg} \\frac{dg}{dx} $$ For a more complex composition $$y = f(g(h(x))) = f(g(h(x_1))) = f(g(x_2)) = f(x_3) = x_4$$ the chain rule gives   \n",
        "    $$\\frac{dy}{dx} = \\frac{dy}{dx_3} \\frac{dx_3}{dx_2} \\frac{dx_2}{dx_1}\\frac{dx_1}{dx}$$  \n",
        "    \n",
        " * **Forward Accumulation**:  \n",
        " We can think of taking derivatives in AD as building a graph representing evaluation of a function, and using the chain rule to propagate derivative values from input variables to the final function, where each node represents an elementary operation or elementary function.\n",
        " \n",
        " Below is a sample such graph structure resulting from evaluating the function $$f(x_1,x_2) = \\sin(x_1)  + x_1 x_2$$\n",
        " \n",
        " ![](https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png)\n",
        " [image source](https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png)\n",
        " \n",
        " We see that the input values $x_1,x_2$ and their derivatives $\\dot{w_1}, \\dot{w_2}$ propagate upward and are used in computations for the derivatives $\\dot{w_3}, \\dot{w_4}, \\dot{w_5}$ at each node on the way up to the final function $f(x_1,x_2) = w_5$. \n",
        " \n",
        "<!--  ![alt text](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png =400x200)   \\\\ -->\n",
        " \n",
        " * **Reverse Accumulation**:  \n",
        " In reverse accumulation AD, one first fixes the dependent variable to be differentiated and computes the derivative with respect to each sub-expression recursively. It traverses the chain rule from outside to inside.\n",
        " \n",
        " Below is the same graph structure for $f(x_1, x_2)$, but with reverse propagation notated instead.\n",
        " \n",
        " ![](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png)\n",
        " [image source](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png)\n",
        " \n",
        " * **AD in Linear Regression**:  \n",
        " Linear regression model is modeling a dependent variable linearly dependent on some set of independent variables in a noisy environment. $$y^{(i)} = \\boldsymbol{\\theta}^{T}\\boldsymbol{x}^{(i)} + \\epsilon^{(i)} $$\n",
        "We can write the likelihood function given all the observations as: $$ \\mathcal{L}(\\boldsymbol{\\theta}; X, \\boldsymbol{y}) =  \\prod_{i = 1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\big(\\frac{-(y^{(i)} - \\boldsymbol{\\theta}^{T}\\boldsymbol{x}^{(i)})^2}{2\\sigma^2}\\big)  $$\n",
        "In order to find the best fitting parameters $\\boldsymbol{\\theta}$ we therefore need to maximize this function with respect to $\\boldsymbol{\\theta}$. The standard approach is to maximize the log likelihood which, since log is monotonic, will give the same result. Hence maximizing the likelihood is the same as minimizing the estimate of the variance which is also known as loss function.  $$\\mathcal{J}(\\boldsymbol{\\theta}) = \\sum_{i=1}^n (y^{(i)} - \\boldsymbol{\\theta}^{T}\\boldsymbol{x}^{(i)})^2$$ \n",
        "In order to mininize the cost function, we use the method of steepest descent$$   \\boldsymbol{\\theta}^{i+1} = \\boldsymbol{\\theta}^{i} - \\alpha \\nabla\\mathcal{J}(\\boldsymbol{\\theta}) $$\n",
        "where we can use automatic differentiation."
      ]
    },
    {
      "metadata": {
        "id": "BozZIls9KZRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Software Organization\n",
        "\n",
        "## Directory Structure\n",
        "We keep the old implementation in one of\n",
        "the branches and not test for it, since we made significant changes to our old implementation and do not use the old one anymore. Our new\n",
        "implementation is all in 'code' branch.\n",
        "\n",
        "Our directory structure looks like the following:\n",
        "* `lazydiff/`\n",
        "     * `__init__.py`\n",
        "     * `vars.py`\n",
        "     * `ops.py`\n",
        "     * `regression.py`\n",
        "     * `tests/`\n",
        "       * `__init__.py`\n",
        "       * `test_scalar.py`\n",
        "       * `test_ops.py`\n",
        "       * `test_vector.py`\n",
        "       * `test_vector_ops.py`\n",
        "       * `test_regression.py`\n",
        "* `demo/`\n",
        " * `Demo.mov`\n",
        " * `demo_code.ipynb`\n",
        " * `new_implementation.png`\n",
        " * `old_implementation.png`\n",
        " * `presentation.ipynb`\n",
        " * `presentation.slides.html`\n",
        "* `docs/`\n",
        " * `milestone1.ipynb`\n",
        " * `milestone2.ipynb`\n",
        " * `documentation.ipynb`\n",
        "* `README.md`\n",
        "* `LICENSE`\n",
        "* `setup.py`\n",
        "* `requirements.txt`\n",
        "* `.travis.yml`\n",
        "* `.coveragerc`\n",
        "* `setup.cfg`\n",
        "\n",
        "## Basic Modules\n",
        "All the modules related to autodiff are placed inside the `lazydiff` folder. Files outside this folder are miscellaneous files for setups, Github or test coverages.\n",
        "\n",
        "The key modules included are `vars.py`, which contains our base Var class as a wrapper of numpy arrays that can handle both scalars and vectors and built-in functions for which we may want to compute, and `ops.py`, which contains methods that allow us to evaluate elementary functions such as $\\sin$, $\\cos$, etc.\n",
        "\n",
        "In addition, a module `regression.py` is included as a demonstration of our autodiff capabilities.\n",
        "\n",
        "## Tests\n",
        "The test suite live in the folder \"tests\" in the directory structure above. We use TravicsCI for continuous integration and Coveralls for test coverage.\n",
        "\n",
        "## Install Package\n",
        "The packaged library is distributed through Github which can be installed using the pip command and the tutorial is available above in the `How to Install` section."
      ]
    },
    {
      "metadata": {
        "id": "vdEKrB2oKZxB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "## Core Data Structures\n",
        "\n",
        "The core of this library relies on the idea of variables, which store a float for the value it represents; a dictionary of previosly computed gradient values given variable reference as key; a dictionary of the variable's parents' gradients given parent variable reference as key and a dictionary of the gradients of variable's children given children variable reference as key.\n",
        "\n",
        "Parents dictionary is used in forward mode and children dictionary is used in reverse mode. Also, through the above data structure, we can easily find the gradient with respect to any past variable used without recomputing it each time. Please see the demo for an example of this. \n",
        "\n",
        "The numpy based implemnetation can handle both scalars and vectors. All the operations are performed on each entry in a vector.\n",
        "\n",
        "## Core Classes and Important Attributes\n",
        "\n",
        "* In module `vars` :\n",
        "    * We have a class `Var` which contains:\n",
        "        * Constructor which takes in a value `val`\n",
        "            * Initializes empty gradient dictionary `grad_val`, which will contain derivatives of this variable with respect to other variables.\n",
        "            * Initializes dictionary representing gradient calculation parents (dictionary with parent variables as keys and gradients of parents as values). For instance, if we are evaluating the derivative of $\\sin x$ at $x = 3$, $$\\frac{d}{dx} \\sin x \\Big|_{x = 3}= \\dot{x}|_{x=3} \\cos 3$$ so the dictionary of object $\\sin x$ would include the the item $\\{x: \\cos 3\\}$.\n",
        "            * Initializes dictionary representing gradient calculation children (dictionary with child variables as keys and gradients of children as values). For instance, if we are evaluating the derivative of $\\sin x$ at $x = 3$, $$\\frac{d}{dx} \\sin x \\Big|_{x = 3}= \\dot{x}|_{x=3} \\cos 3$$ so the dictionary of object $x$ would include the the item $\\{\\sin x: \\cos 3\\}$.\n",
        "        * Method ```forward``` which visits all descendants of a given variable and then calculates the gradient of each descendant with respect to the given variable, and stores them into `grad_val` dictionary. \n",
        "        * Method ```backward``` which visits all ancestors of a given variable and then calculates the gradient of the given variable with respect to each ancestor, and stores them into `grad_val` dictionary. \n",
        "        * Method ```grad``` which takes in a variable, and returns a numpy array containing the derivative with respect to that variables. This will only work if the appropriate forward/backward calls have been made. \n",
        "        * Methods `__add__`, `__mul__`, etc and comparison operators for overloading basic arithmetic operations.\n",
        "      \n",
        "## Elementary functions\n",
        "\n",
        "* In module `ops`:\n",
        "    * We have a method for each elementary function (`trig functions`, `inverse trig functions`, `exponentials`, `hyperbolic functions`, `logistic function`, `logarithms` and `square root`, etc.). Each method:\n",
        "        * Take in a `Var` object \n",
        "        * Create a new `Var` object with value updated to reflect execution of elementary function on value of input `Var` object, updated gradient calculation parents and children dictionaries - these compounds functions together\n",
        "        \n",
        "    * We have implemented all the basic arithmetic functions from  `numpy` libraries, by directly calling these functions for evaluating the values. Thus, our `ops` library supports basic elementary functions ranging from `abs` to `asin` etc.\n",
        "    \n",
        "## Additional Modules\n",
        "* In module `regression`:\n",
        " * We have methods for Mean Square Error (`MSE`) and MSE with regularization (`MSE_regularized`) which are loss functions in linear regression. Each method takes in the dependent,  independent variables, coefficients and intercept. `MSE_regularized` takes in the weight in L-p norm of the coefficients vector as well. For regularization part, we implement Lasso, Ridge and Elastic algorithms. \n",
        " * We also have a `gradient_descent` method which performs one single update step of gradient descent and returns the updated parameters m, b, and loss. It takes in the dependent, independent variables, coefficients, intercept and learning rate. Also, it takes in forward/reverse mode as well to define which mode do we want to use. \n",
        "  * `iterative_regression` is a method with a timer decorator performing iterative regression with the given loss function and minizing the loss function with respect to the parameters. We are able to compare the speed of forward and backward due to the timer we used. It takes in the dependent, independent variables, coefficients, intercept, learning rate, epochs, number of iterations for early stopping and forward/reverse mode. \n",
        "    \n",
        "## External Dependencies\n",
        "\n",
        "The implementations rely on `numpy` for evaluating elementary functions, such as cos, sin, tan, and use numpy arrays to store variable values."
      ]
    },
    {
      "metadata": {
        "id": "lIurSRCWAW6X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Future\n",
        "\n",
        "## Option for higher-order derivatives (Hessians and beyond)\n",
        "\n",
        "We are thinking of supporting higher-order derivatives, such as Hessians. This can be done by further extending the Vector class with additional methods named as `.Hessian(variables)`. The main challenges of this extension is to figure out how to utilize `.grad(variables)` method to effectively get Hessians or beyond in a recursive manner to prevent reusing duplicate codes. Another challenge is how to do thorough testings of different operations on Hessian and beyond, which can be quite labor intensive.\n",
        "\n",
        "## More application modules\n",
        "\n",
        "We currently have a module `regression` that allows a user to easily use lazydiff for some basic regression tasks, but it would be nice if we could add more modules to support solving other optimization problems with lazydiff as well."
      ]
    }
  ]
}