{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqirvYUFggX9"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this project is to develop a library for performing automatic differentiation (AD), which we will call ```LazyDiff```.\n",
    "AD is a set of techniques to numerically evaluate the derivative of a function motivated by deficiencies in classical methods. Numerical approximation is often not accurate enough and can be computationally expensive and symbolic differentiation tends to lead to inefficient code and faces the difficulty of converting a computer program into a single expression. Both classical methods have problems with calculating higher derivatives, where the complexity and errors increase, and are slow when it comes to computing the partial derivatives of a function with respect to many inputs. Automatic differentiation endeavors to solve all of these problems, and has been used in neural networks for back propagation and weight adjustment based on the loss function, and in scientific computing where it is often difficult to analytically compute the derivatives.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-b0Gl2LrkBT"
   },
   "source": [
    "# How to Use LazyDiff\n",
    "\n",
    "## How to Install\n",
    "\n",
    "The easy way using pip:\n",
    "\n",
    "- `pip install lazydiff`\n",
    "\n",
    "You can also execute following command to install LazyDiff and its dependencies:\n",
    "\n",
    "- `pip install git+https://github.com/CS207-Project-Group-7/cs207-FinalProject.git`\n",
    "\n",
    "If you are cloning our repo and installing lazydiff that way, you can run the following to install:\n",
    "\n",
    "- `python3 setup.py install`\n",
    "\n",
    "and run the following to run our tests:\n",
    "\n",
    "- `python3 setup.py test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "Ye-aZbj7JTem",
    "outputId": "db54dc2e-2d10-41b5-cda3-53e88a108b4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lazydiff\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/cc/2501134605442b8457f3eb1c8779ec9f2c4bc0bf315637bfc40b855fb887/lazydiff-0.1-py3-none-any.whl\n",
      "Collecting numpy (from lazydiff)\n",
      "  Using cached https://files.pythonhosted.org/packages/3d/c3/a69406093c9a780a74964f41cd56b06c0346d686a9b3f392d123a663f5e0/numpy-1.15.4-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Installing collected packages: numpy, lazydiff\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\n",
      "  Found existing installation: lazydiff 0.1\n",
      "    Uninstalling lazydiff-0.1:\n",
      "      Successfully uninstalled lazydiff-0.1\n",
      "Successfully installed lazydiff-0.1 numpy-1.15.4\n"
     ]
    }
   ],
   "source": [
    "# Install lazydiff for demos\n",
    "!pip install --force-reinstall lazydiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGcNOcxkJPD6"
   },
   "source": [
    "## Basic Demo\n",
    " \n",
    " ### Variable Objects and Evaluating Functions \n",
    " \n",
    " The values of variables are stored inside `vars` objects as `Var`, which is used as function inputs. Custom functions from `ops` module need to be used to evaluate the functions, which support all the standard math functions from math and numpy libraries. \n",
    " \n",
    " Function values can be evaluated by simply wrapping the variables with the corresponding functions and accessing the values using `.val`\n",
    " \n",
    "Below we show the examples for basic operations and some of our elementary functions. \n",
    "\n",
    "We evaluate $f(x) = x^2$ and $f(y) = -y$ for variables $x=4$ and $y=2$. We also show how we can compound the functions by evaluating $f(z) = 5*z$ where $z = f(x) + f(y)$, $z = f(x) - f(y)$ and $z = \\frac{f(x)}{f(y)}$, respectively.\n",
    " \n",
    " Also, we evaluate $f(a) = sin(a)$ for $a = 0$,  $f(b) = arcsin(b)$ for $b=0$, and $f(c) = \\frac{1}{1 + e^{-{c}}} + log_2(c)+ \\sqrt{c}$  for $c = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "vCt7eKNDvsqv",
    "outputId": "14e5ead4-b340-45cc-e92a-e51833eaf9b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating f(x) = x^2 where x = 4 : 16.0\n",
      "Evaluating f(y) = -y where y = 2 : -2.0\n",
      "Evaluating f(z) = 5*z where z = f(x) + f(y) : 70.0\n",
      "Evaluating f(z) = 5*z where z = f(x) - f(y) : 90.0\n",
      "Evaluating f(z) = 5*z where z = f(x) / f(y) : -40.0\n",
      "Evaluating f(a) = sin(a) where a = 0 : 0.0\n",
      "Evaluating f(b) = arcsin(b) where b = 0 : 0.0\n",
      "Evaluating f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) where c = 1 : 1.7310585786300048\n"
     ]
    }
   ],
   "source": [
    "from lazydiff.vars import Var\n",
    "from lazydiff import ops\n",
    "import math\n",
    "\n",
    "#part 1\n",
    "# creating variable x = 4 and y = 2\n",
    "x = Var(4)\n",
    "y = Var(2)\n",
    "\n",
    "# constructing and evaluating f(x) = x^2 and f(y) = sin(y)\n",
    "fx = x**2\n",
    "fy = -y\n",
    "# compound function f(z)_1 = 5*z where z = fx + fy\n",
    "fz_1 = 5*(fx + fy)\n",
    "# compound function f(z)_2 = 5*z where z = fx - fy\n",
    "fz_2 = 5*(fx - fy)\n",
    "# compound function f(z)_3 = 5*z where z = fx / fy\n",
    "fz_3 = 5*(fx / fy)\n",
    "\n",
    "# accessing the evaluated values\n",
    "# output should print out 16, -2, 70, 90 and -40\n",
    "print(\"Evaluating f(x) = x^2 where x = 4 : {}\".format(fx.val))\n",
    "print(\"Evaluating f(y) = -y where y = 2 : {}\".format(fy.val))\n",
    "print(\"Evaluating f(z) = 5*z where z = f(x) + f(y) : {}\".format(fz_1.val))\n",
    "print(\"Evaluating f(z) = 5*z where z = f(x) - f(y) : {}\".format(fz_2.val))\n",
    "print(\"Evaluating f(z) = 5*z where z = f(x) / f(y) : {}\".format(fz_3.val))\n",
    "\n",
    "#part 2\n",
    "# creating variable a = 0, b = 0 and c = 1\n",
    "a = Var(0)\n",
    "b = Var(0)\n",
    "c = Var(1)\n",
    "\n",
    "# constructing and evaluating f(a) = sin(a), f(b) = arcsin(b) and f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2)\n",
    "fa = ops.sin(a)\n",
    "fb = ops.arcsin(b)\n",
    "fc = ops.logistic(c) + ops.log(c, 2) + ops.sqrt(c)\n",
    "# accessing the evaluated values\n",
    "# output should print out 0, 0 and 1.73\n",
    "print(\"Evaluating f(a) = sin(a) where a = 0 : {}\".format(fa.val))\n",
    "print(\"Evaluating f(b) = arcsin(b) where b = 0 : {}\".format(fb.val))\n",
    "print(\"Evaluating f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) where c = 1 : {}\".format(fc.val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_JlykEZyfhr"
   },
   "source": [
    "### Evaluating Function Gradients\n",
    "\n",
    "#### Forward Mode\n",
    "\n",
    "As each function is evaluated, the corresponding gradient is also stored in the new output variable. Therefore, each of the output variable contains the gradients from all the previously evaluated functions. This allows us to access the gradient with respect to any of the variables previously computed. For instance, for $f(z)$ from the above example we can compute the gradient with respect to $z$ or $x$ depending on what we pass as input to the gradient method. This also makes it necessary to store the variables of interest somewhere in the program and then use the variable for which we want to evaluate the gradient of as input to the gradient method of the function. \n",
    "\n",
    "Below we show an example of how to access gradients of $f(x)$, $f(y)$, $f(z)$ and $f(a)$, $f(b)$, $f(c)$using `.grad(variable)` method via forward mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "BAFV93rRBOQH",
    "outputId": "883aaf5c-2d5b-4794-ed85-0c75ab5a8a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of f(x) = x^2 w.r.t x is 8.0\n",
      "Gradient of f(y) = -y w.r.t y is -1.0\n",
      "Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t x is 40.0\n",
      "Gradients of f(z) = 5*z where z = f(x) - f(y) w.r.t x is 40.0\n",
      "Gradients of f(z) = 5*z where z = f(x) / f(y) w.r.t x is -20.0\n",
      "Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t y is -5.0\n",
      "Gradient of f(a) = sin(a) w.r.t a is 1.0\n",
      "Gradient of f(b) = arcsin(b) w.r.t y is 1.0\n",
      "Gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is 2.139306974130445\n"
     ]
    }
   ],
   "source": [
    "# accessing gradients\n",
    "#part 1\n",
    "# gradient of f(x) = x^2 w.r.t x is 8\n",
    "x.forward()\n",
    "print(\"Gradient of f(x) = x^2 w.r.t x is {}\".format(fx.grad(x)))\n",
    "# gradient of f(y) = -y w.r.t y is -1\n",
    "y.forward()\n",
    "print(\"Gradient of f(y) = -y w.r.t y is {}\".format(fy.grad(y)))\n",
    "\n",
    "# gradient of fz_1 w.r.t x is d5(x^2-y)/dx = 10x = 40\n",
    "print(\"Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t x is {}\".format(fz_1.grad(x)))\n",
    "# gradient of fz_2 w.r.t x is d5(x^2+y)/dx = 10x = 40\n",
    "print(\"Gradients of f(z) = 5*z where z = f(x) - f(y) w.r.t x is {}\".format(fz_2.grad(x)))\n",
    "# gradient of fz_3 w.r.t x is d5(x^2/(-y))/dx = -5x = -20\n",
    "print(\"Gradients of f(z) = 5*z where z = f(x) / f(y) w.r.t x is {}\".format(fz_3.grad(x)))\n",
    "\n",
    "# illustratation of taking two variables as input \n",
    "# gradient of fz_1 w.r.t y is d5(x^2-y)/dy = -5\n",
    "print(\"Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t y is {}\".format(fz_1.grad(y)))\n",
    "\n",
    "#part 2\n",
    "# gradient of f(a) = sin(a) w.r.t a is 1\n",
    "a.forward()\n",
    "print(\"Gradient of f(a) = sin(a) w.r.t a is {}\".format(fa.grad(a)))\n",
    "# gradient of f(b) = arcsin(b) w.r.t b is 1\n",
    "b.forward()\n",
    "print(\"Gradient of f(b) = arcsin(b) w.r.t y is {}\".format(fb.grad(b)))\n",
    "# gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is 2.14\n",
    "c.forward()\n",
    "print(\"Gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is {}\".format(fc.grad(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1L_5GtALDJhK"
   },
   "source": [
    "#### Reverse Mode\n",
    "\n",
    "Below we show an example of how to access gradients of $f(x)$, $f(y)$, $f(z)$ and $f(a)$, $f(b)$, $f(c)$using `.grad(variable)` method via reverse mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "SWtLbtC_DJAJ",
    "outputId": "93a6216b-595c-4241-967b-2b17a4d089de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of f(x) = x^2 w.r.t x is 8.0\n",
      "Gradient of f(y) = -y w.r.t y is -1.0\n",
      "Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t x is 40.0\n",
      "Gradients of f(z) = 5*z where z = f(x) - f(y) w.r.t x is 40.0\n",
      "Gradients of f(z) = 5*z where z = f(x) / f(y) w.r.t x is -20.0\n",
      "Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t y is -5.0\n",
      "Gradient of f(a) = sin(a) w.r.t a is 1.0\n",
      "Gradient of f(b) = arcsin(b) w.r.t y is 1.0\n",
      "Gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is 2.139306974130445\n"
     ]
    }
   ],
   "source": [
    "# accessing gradients\n",
    "#part 1\n",
    "# gradient of f(x) = x^2 w.r.t x is 8\n",
    "fx.backward()\n",
    "print(\"Gradient of f(x) = x^2 w.r.t x is {}\".format(fx.grad(x)))\n",
    "# gradient of f(y) = -y w.r.t y is -1\n",
    "fy.backward()\n",
    "print(\"Gradient of f(y) = -y w.r.t y is {}\".format(fy.grad(y)))\n",
    "\n",
    "# gradient of fz_1 w.r.t x is d5(x^2-y)/dx = 10x = 40\n",
    "fz_1.backward()\n",
    "print(\"Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t x is {}\".format(fz_1.grad(x)))\n",
    "# gradient of fz_2 w.r.t x is d5(x^2+y)/dx = 10x = 40\n",
    "fz_2.backward()\n",
    "print(\"Gradients of f(z) = 5*z where z = f(x) - f(y) w.r.t x is {}\".format(fz_2.grad(x)))\n",
    "# gradient of fz_3 w.r.t x is d5(x^2/(-y))/dx = -5x = -20\n",
    "fz_3.backward()\n",
    "print(\"Gradients of f(z) = 5*z where z = f(x) / f(y) w.r.t x is {}\".format(fz_3.grad(x)))\n",
    "\n",
    "# illustratation of taking two variables as input \n",
    "# gradient of fz_1 w.r.t y is d5(x^2-y)/dy = -5\n",
    "print(\"Gradients of f(z) = 5*z where z = f(x) + f(y) w.r.t y is {}\".format(fz_1.grad(y)))\n",
    "\n",
    "#part 2\n",
    "# gradient of f(a) = sin(a) w.r.t a is 1\n",
    "fa.backward()\n",
    "print(\"Gradient of f(a) = sin(a) w.r.t a is {}\".format(fa.grad(a)))\n",
    "# gradient of f(b) = arcsin(b) w.r.t b is 1\n",
    "fb.backward()\n",
    "print(\"Gradient of f(b) = arcsin(b) w.r.t y is {}\".format(fb.grad(b)))\n",
    "# gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is 2.14\n",
    "fc.backward()\n",
    "print(\"Gradient of f(c) = 1/(1+e^(-c)) + log2(c) + c^(1/2) w.r.t c is {}\".format(fc.grad(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFRWl-Dl3X3Q"
   },
   "source": [
    "### Working with Vectors\n",
    "\n",
    "Our class Var is a wrapper for numpy arrays that can handle both scalars and vectors.\n",
    "\n",
    "Below we show an example for evaluating the value and the gradients/Jacobian of $f(z) = 2*z$ where $z = f(x,y)$ and  $f(x,y) = x^2+y$ where $x$,$y$ are the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "bxegV6eX-BOh",
    "outputId": "5a02586d-8554-4ab8-cce1-3b878c587cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating f(x,y) = x^2 + y : [ 2.  6. 12. 20. 30.]\n",
      "Evaluating f(z) = 2*z : [ 4. 12. 24. 40. 60.]\n",
      "Jacobian of f(x,y) w.r.t. x:\n",
      "[ 2.  4.  6.  8. 10.]\n",
      "Jacobian of f(z) w.r.t. x:\n",
      "[ 4.  8. 12. 16. 20.]\n",
      "Jacobian of f(z) w.r.t. y:\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# creating vector variables x = (1, 2, 3, 4, 5) y = (1, 2, 3, 4, 5)\n",
    "x = Var([1, 2, 3, 4, 5])\n",
    "y = Var([1, 2, 3, 4, 5])\n",
    "\n",
    "# constructing and evaluating f(x,y) and f(z)\n",
    "fxy = x**2 + y\n",
    "fz = 2*fxy\n",
    "\n",
    "# accessing the evaluated values\n",
    "print(\"Evaluating f(x,y) = x^2 + y : {}\".format(fxy.val))\n",
    "print(\"Evaluating f(z) = 2*z : {}\".format(fz.val))\n",
    "\n",
    "# Jacobian of f(x,y) w.r.t x\n",
    "x.forward()\n",
    "print(\"Jacobian of f(x,y) w.r.t. x:\\n{}\".format(fxy.grad(x)))\n",
    "# Jacobian of f(z) w.r.t x\n",
    "print(\"Jacobian of f(z) w.r.t. x:\\n{}\".format(fz.grad(x)))\n",
    "# Jacobian of f(z) w.r.t y\n",
    "y.forward()\n",
    "print(\"Jacobian of f(z) w.r.t. y:\\n{}\".format(fz.grad(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OL45Pg0d7shj"
   },
   "source": [
    "### Easy-To-Use Newton's Method with lazydiff\n",
    "\n",
    "We show how we can use Newton's Method to solve for root of a question using lazzydiff\n",
    "\n",
    "In this case, we explore this using $f(x) = x^2 - 2$, which has a root at $x=\\sqrt2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "P8_gX--H8Ere"
   },
   "outputs": [],
   "source": [
    "def Newton_Method(f, x0, maxiter=1000):\n",
    "    \"\"\" returns the root for the given function\n",
    "        using Newtons Method\n",
    "        \n",
    "        this function supports only Scalar\n",
    "        you can modify this function to support Vector\n",
    "        \n",
    "        ===INPUT===\n",
    "        f: function written in ops or built_in arithmetic functions\n",
    "        x0: initial point\n",
    "        maxiter: maximum number of iterations\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for _ in range(maxiter):\n",
    "        if abs(f(x)).val < 1e-8:\n",
    "            return x\n",
    "        fx = f(x)\n",
    "        fx.backward()\n",
    "        newx = x - fx/fx.grad(x)\n",
    "        if (abs(newx-x).val < 1e-8):\n",
    "            return newx\n",
    "        x = newx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "q3_pFeX68jOU",
    "outputId": "bac66e9f-17d8-409b-c73d-e7b0add23088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root found by Newton's Method using autodiff is: 1.4142135623746899\n",
      "The difference between this root and the real root 2**0.5 is : 1.5947243525715749e-12\n"
     ]
    }
   ],
   "source": [
    "# the function x^2-2 for which we want to find the root of\n",
    "fx = lambda x: x**2-2\n",
    "ans = Newton_Method(fx, Var(1))\n",
    "print(\"The root found by Newton's Method using autodiff is: {}\".format(ans.val))\n",
    "print(\"The difference between this root and the real root 2**0.5 is : {}\".format(ans.val-2**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: Regression and Backpropagation\n",
    "\n",
    "Please see `demo_code.ipynb` notebook under `demo` folder for a detailed walkthrough of the extension package.\n",
    "\n",
    "Here we only explore how to perform Linear Regression and Polynomial Regression, and validate the results against the ground truth or sklearn models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data\n",
    "\n",
    "Let's first create a synthetic dataset using make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lazydiff import regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "dim = 1\n",
    "X,y,true_coef = make_regression(n_samples = 100, n_features = dim, n_informative = dim, bias = 10, \\\n",
    "                                coef = True, noise = 0, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation & Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is used when we set the variable `forward` to `False`.\n",
    "\n",
    "Now let's model linear regression. All we need to do is use `regression.MSE` as loss function when passing to our training function, `regression.iterative_regression`. \n",
    "\n",
    "Similarly, for Lasso, Ridge and Elastic Net, we only need to replace the loss function with `regression.lasso_loss`, `regression.ridge_loss`, `regression.elastic_loss` respectively,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Var(np.ones(X.shape[1]))\n",
    "b = Var(0)\n",
    "earlyStop = 0\n",
    "forward = False\n",
    "linear_history = {}\n",
    "m, b, loss = regression.iterative_regression(X, y, m, b, regression.MSE, 0.1, 100,\n",
    "                                                    earlyStop, forward, linear_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between iterative and sklearn linear regression\n",
      "Coefficient: [-3.82769228e-06]\n",
      "Intercept: 1.0089887698683242e-06\n"
     ]
    }
   ],
   "source": [
    "clf = LinearRegression().fit(X,y)\n",
    "## We see that our implementation of Linear Regression \n",
    "## produces approximately the same results as the Sklearn model.\n",
    "print(\"Difference between iterative and sklearn linear regression\")\n",
    "print(\"Coefficient: {}\".format(m.val - clf.coef_))\n",
    "print(\"Intercept: {}\".format(b.val - clf.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Polynomial regression can also be easily implemented by extending the dataset `X` with polynomial terms using `PolynomialFeatures`.\n",
    "\n",
    "Let's test this for $y = x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different between predicted y and observed y is [-0.00102747 -0.00042329  0.00011376  0.00058368  0.00098646  0.00132212\n",
      "  0.00159064  0.00179203  0.0019263   0.00199343  0.00199343  0.0019263\n",
      "  0.00179203  0.00159064  0.00132212  0.00098646  0.00058368  0.00011376\n",
      " -0.00042329 -0.00102747]\n"
     ]
    }
   ],
   "source": [
    "# producting data\n",
    "X = np.linspace(-10,10,20).reshape(-1,1)\n",
    "y = X.reshape(-1)**2\n",
    "X = PolynomialFeatures(2, include_bias = False).fit_transform(X)\n",
    "\n",
    "# fitting the data\n",
    "m = Var(np.random.rand(X.shape[1]), seed = 1.0)\n",
    "b = Var(0)\n",
    "earlyStop = 0\n",
    "forward = False\n",
    "m, b, loss = regression.iterative_regression(X, y, m, b, regression.MSE, 0.0001, 10000,\n",
    "                                                    earlyStop, forward)\n",
    "predict = np.sum(m.val*X, axis=1)+b.val\n",
    "\n",
    "## The prediction is also very close to the observed y!\n",
    "print(\"Different between predicted y and observed y is {}\".format(predict-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvK8XHql_9Q7"
   },
   "source": [
    "# Background\n",
    " AD exploits the fact that every function, no matter how complicated, is a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (such as $\\sin$, $\\cos$, $\\exp$, $\\log$, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
    " * **Chain Rule**  \n",
    "   - AD relies heavily on the chain rule from calculus, which says that $$\\frac{d}{dx} f(g(x)) = \\frac{df}{dg} \\frac{dg}{dx} $$ For a more complex composition $$y = f(g(h(x))) = f(g(h(x_1))) = f(g(x_2)) = f(x_3) = x_4$$ the chain rule gives   \n",
    "    $$\\frac{dy}{dx} = \\frac{dy}{dx_3} \\frac{dx_3}{dx_2} \\frac{dx_2}{dx_1}\\frac{dx_1}{dx}$$  \n",
    "    \n",
    " * **Forward Accumulation**:  \n",
    " We can think of taking derivatives in AD as building a graph representing evaluation of a function, and using the chain rule to propagate derivative values from input variables to the final function, where each node represents an elementary operation or elementary function.\n",
    " \n",
    " Below is a sample such graph structure resulting from evaluating the function $$f(x_1,x_2) = \\sin(x_1)  + x_1 x_2$$\n",
    " \n",
    " ![](https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png)\n",
    " [image source](https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png)\n",
    " \n",
    " We see that the input values $x_1,x_2$ and their derivatives $\\dot{w_1}, \\dot{w_2}$ propagate upward and are used in computations for the derivatives $\\dot{w_3}, \\dot{w_4}, \\dot{w_5}$ at each node on the way up to the final function $f(x_1,x_2) = w_5$. \n",
    " \n",
    "<!--  ![alt text](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png =400x200)   \\\\ -->\n",
    " \n",
    " * **Reverse Accumulation**:  \n",
    " In reverse accumulation AD, one first fixes the dependent variable to be differentiated and computes the derivative with respect to each sub-expression recursively. It traverses the chain rule from outside to inside.\n",
    " \n",
    " Below is the same graph structure for $f(x_1, x_2)$, but with reverse propagation notated instead.\n",
    " \n",
    " ![](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png)\n",
    " [image source](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png)\n",
    " \n",
    " * **AD in Linear Regression**:  \n",
    " Linear regression model is modeling a dependent variable linearly dependent on some set of independent variables in a noisy environment. $$y^{(i)} = \\boldsymbol{\\theta}^{T}\\boldsymbol{x}^{(i)} + \\epsilon^{(i)} $$\n",
    "We can write the likelihood function given all the observations as: $$ \\mathcal{L}(\\boldsymbol{\\theta}; X, \\boldsymbol{y}) =  \\prod_{i = 1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\big(\\frac{-(y^{(i)} - \\boldsymbol{\\theta}^{T}\\boldsymbol{x}^{(i)})^2}{2\\sigma^2}\\big)  $$\n",
    "In order to find the best fitting parameters $\\boldsymbol{\\theta}$ we therefore need to maximize this function with respect to $\\boldsymbol{\\theta}$. The standard approach is to maximize the log likelihood which, since log is monotonic, will give the same result. Hence maximizing the likelihood is the same as minimizing the estimate of the variance which is also known as loss function.  $$\\mathcal{J}(\\boldsymbol{\\theta}) = \\sum_{i=1}^n (y^{(i)} - \\boldsymbol{\\theta}^{T}\\boldsymbol{x}^{(i)})^2$$ \n",
    "In order to mininize the cost function, we use the method of steepest descent$$   \\boldsymbol{\\theta}^{i+1} = \\boldsymbol{\\theta}^{i} - \\alpha \\nabla\\mathcal{J}(\\boldsymbol{\\theta}) $$\n",
    "where we can use automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BozZIls9KZRv"
   },
   "source": [
    "# Software Organization\n",
    "\n",
    "## Directory Structure\n",
    "We keep the old implementation in one of\n",
    "the branches and not test for it, since we made significant changes to our old implementation and do not use the old one anymore. Our new\n",
    "implementation is all in 'master' branch.\n",
    "\n",
    "Our directory structure looks like the following:\n",
    "* `lazydiff/`\n",
    "     * `__init__.py`\n",
    "     * `vars.py`\n",
    "     * `ops.py`\n",
    "     * `regression.py`\n",
    "     * `tests/`\n",
    "       * `__init__.py`\n",
    "       * `test_ops.py`\n",
    "       * `test_vars_backward.py`\n",
    "       * `test_vars_forward.py`\n",
    "       * `test_vector_ops.py`\n",
    "       * `test_vector_backward.py`\n",
    "       * `test_vector_forward.py`\n",
    "       * `test_regression.py`\n",
    "* `demo/`\n",
    " * `Demo.mov`\n",
    " * `demo_2.mov`\n",
    " * `demo_code.ipynb`\n",
    " * `new_implementation.png`\n",
    " * `old_implementation.png`\n",
    " * `presentation.ipynb`\n",
    " * `presentation.slides.html`\n",
    "* `docs/`\n",
    " * `milestone1.ipynb`\n",
    " * `milestone2.ipynb`\n",
    " * `Documentation.ipynb`\n",
    "* `README.md`\n",
    "* `LICENSE`\n",
    "* `setup.py`\n",
    "* `requirements.txt`\n",
    "* `.travis.yml`\n",
    "* `.coveragerc`\n",
    "* `setup.cfg`\n",
    "\n",
    "## Basic Modules\n",
    "All the modules related to autodiff are placed inside the `lazydiff` folder. Files outside this folder are miscellaneous files for setups, Github or test coverages.\n",
    "\n",
    "The key modules included are `vars.py`, which contains our base Var class as a wrapper of numpy arrays that can handle both scalars and vectors and built-in functions for which we may want to compute, and `ops.py`, which contains methods that allow us to evaluate elementary functions such as $\\sin$, $\\cos$, etc.\n",
    "\n",
    "\n",
    "\n",
    "## Tests\n",
    "The test suite live in the folder \"tests\" in the directory structure above. We use TravicsCI for continuous integration and Coveralls for test coverage.\n",
    "\n",
    "## Install Package\n",
    "The packaged library is distributed through PyPl and Github which can be installed using the pip command and the tutorial is available above in the `How to Install` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdEKrB2oKZxB"
   },
   "source": [
    "# Implementation\n",
    "\n",
    "## Core Data Structures\n",
    "\n",
    "The core of this library relies on the idea of variables, which store a float for the value it represents; a dictionary of previosly computed gradient values given variable reference as key; a dictionary of the variable's parents' gradients given parent variable reference as key and a dictionary of the gradients of variable's children given children variable reference as key.\n",
    "\n",
    "Parents dictionary is used in forward mode and children dictionary is used in reverse mode. Also, through the above data structure, we can easily find the gradient with respect to any past variable used without recomputing it each time. Please see the demo for an example of this. \n",
    "\n",
    "The numpy based implemnetation can handle both scalars and vectors. All the operations are performed on each entry in a vector.\n",
    "\n",
    "## Core Classes and Important Attributes\n",
    "\n",
    "* In module `vars` :\n",
    "    * We have a class `Var` which contains:\n",
    "        * Constructor which takes in a value `val`\n",
    "            * Initializes empty gradient dictionary `grad_val`, which will contain derivatives of this variable with respect to other variables.\n",
    "            * Initializes dictionary representing gradient calculation parents (dictionary with parent variables as keys and gradients of parents as values). For instance, if we are evaluating the derivative of $\\sin x$ at $x = 3$, $$\\frac{d}{dx} \\sin x \\Big|_{x = 3}= \\dot{x}|_{x=3} \\cos 3$$ so the dictionary of object $\\sin x$ would include the the item $\\{x: \\cos 3\\}$.\n",
    "            * Initializes dictionary representing gradient calculation children (dictionary with child variables as keys and gradients of children as values). For instance, if we are evaluating the derivative of $\\sin x$ at $x = 3$, $$\\frac{d}{dx} \\sin x \\Big|_{x = 3}= \\dot{x}|_{x=3} \\cos 3$$ so the dictionary of object $x$ would include the the item $\\{\\sin x: \\cos 3\\}$.\n",
    "        * Method ```forward``` which visits all descendants of a given variable and then calculates the gradient of each descendant with respect to the given variable, and stores them into `grad_val` dictionary. \n",
    "        * Method ```backward``` which visits all ancestors of a given variable and then calculates the gradient of the given variable with respect to each ancestor, and stores them into `grad_val` dictionary. \n",
    "        * Method ```grad``` which takes in a variable, and returns a numpy array containing the derivative with respect to that variables. This will only work if the appropriate forward/backward calls have been made. \n",
    "        * Methods `__add__`, `__mul__`, etc and comparison operators for overloading basic arithmetic operations.\n",
    "      \n",
    "## Elementary functions\n",
    "\n",
    "* In module `ops`:\n",
    "    * We have a method for each elementary function (`trig functions`, `inverse trig functions`, `exponentials`, `hyperbolic functions`, `logistic function`, `logarithms` and `square root`, etc.). Each method:\n",
    "        * Take in a `Var` object \n",
    "        * Create a new `Var` object with value updated to reflect execution of elementary function on value of input `Var` object, updated gradient calculation parents and children dictionaries - these compounds functions together\n",
    "        \n",
    "    * We have implemented all the basic arithmetic functions from  `numpy` libraries, by directly calling these functions for evaluating the values. Thus, our `ops` library supports basic elementary functions ranging from `abs` to `asin` etc.\n",
    "    \n",
    "## Extension: Additional Modules\n",
    "\n",
    "* Support for backpropagation is implemented inside `vars` module through ```backward```\n",
    "\n",
    "* In module `regression`:\n",
    " * We have methods for Mean Square Error (`MSE`) and MSE with regularization (`MSE_regularized`) which are loss functions in linear regression. Each method takes in the dependent,  independent variables, coefficients and intercept. `MSE_regularized` takes in the weight in L-p norm of the coefficients vector as well. For regularization part, we implement Lasso, Ridge and Elastic algorithms with the corresponding losses - `regression.lasso_loss`, `regression.ridge_loss`, `regression.elastic_loss`. \n",
    " * We also have a `gradient_descent` method which performs one single update step of gradient descent and returns the updated parameters m, b, and loss. It takes in the dependent, independent variables, coefficients, intercept and learning rate. Also, it takes in forward/reverse mode as well to define which mode do we want to use. \n",
    "  * `iterative_regression` performs iterative regression with the given loss function and minizing the loss function with respect to the parameters. It takes in the dependent, independent variables, coefficients, intercept, learning rate, epochs, number of iterations for early stopping and forward/reverse mode. \n",
    "    \n",
    "## External Dependencies\n",
    "\n",
    "The implementations rely on `numpy` for evaluating elementary functions, such as cos, sin, tan, and use numpy arrays to store variable values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIurSRCWAW6X"
   },
   "source": [
    "# Future\n",
    "\n",
    "## Option for higher-order derivatives (Hessians and beyond)\n",
    "\n",
    "We are thinking of supporting higher-order derivatives, such as Hessians. This can be done by further extending the `Var` class with additional methods named as `.Hessian(variables)`. The main challenges of this extension is to figure out how to utilize `.grad(variables)` method to effectively get Hessians or beyond in a recursive manner to prevent reusing duplicate codes. In fact, one the main challenges lies in broadcasting to avoid conflicts while making sure we still take advantage of the performance from the underlying numpy that our class wraps. Another challenge is how to do thorough testings of different operations on Hessian and beyond, which can be quite labor intensive. \n",
    "\n",
    "## More application modules\n",
    "\n",
    "We currently have a module `regression` that allows a user to easily use lazydiff for some basic regression tasks, but it would be nice if we could add more modules to support solving other optimization problems with lazydiff as well. Another nice addition is to implement different optimizers, such as `RMSProp`, `Adam`, rather than just naive Gradient Descent. In fact, this is really helpful because we often had to adjust the learning rate manually given how sentitive gradient descent is to learning rate."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "XqirvYUFggX9",
    "BozZIls9KZRv"
   ],
   "name": "Documentation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
