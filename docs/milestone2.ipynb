{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqirvYUFggX9"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this project is to develop a library for performing automatic differentiation (AD), which we will call ```LazyDiff```.\n",
    "AD is a set of techniques to numerically evaluate the derivative of a function motivated by deficiencies in classical methods. Numerical approximation is often not accurate enough and can be computationally expensive and symbolic differentiation tends to lead to inefficient code and faces the difficulty of converting a computer program into a single expression. Both classical methods have problems with calculating higher derivatives, where the complexity and errors increase, and are slow when it comes to computing the partial derivatives of a function with respect to many inputs. Automatic differentiation endeavors to solve all of these problems, and has been used in neural networks for back propagation and weight adjustment based on the loss function, and in scientific computing where it is often difficult to analytically compute the derivatives.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-b0Gl2LrkBT"
   },
   "source": [
    "# How to Use LazyDiff\n",
    "\n",
    "## How to Install\n",
    "\n",
    "If you want to install the package only for the current virtual environment, run the following commands to create and activate a new virtual env:\n",
    "\n",
    "- Move to the desired working directory\n",
    "- `virtualenv env`\n",
    "- `source env/bin/activate`\n",
    "\n",
    "You can run the following to install the dependencies:\n",
    "- `pip install -r requirements.txt`\n",
    "\n",
    "Then execute following command to install LazyDiff and its dependencies:\n",
    " - `pip install git+https://github.com/CS207-Project-Group-7/cs207-FinalProject.git`\n",
    " \n",
    " \n",
    " ## Basic Demo\n",
    " \n",
    " ### Variable Objects and Evaluating Functions\n",
    " \n",
    " The values of variables are stored inside `vars` objects as `Scalar` or `Vector`, which are used as function inputs. Custom functions from `ops` module need to be used to evaluate the functions, which support all the standard math functions from math and numpy libraries. \n",
    " \n",
    " Function values can be evaluated by simply wrapping the variables with the corresponding functions and accessing the values using `.val`\n",
    " \n",
    "Below we show a simple example for evaluating $f(x) = x^2$ and $f(y) = sin(y)$ for variables $x=3$ and $y=0$. We also show how we can compound the functions by evaluating $f(z) = 5*z$ where $z = f(x)$.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCt7eKNDvsqv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating f(x) = x^2 where x = 3 : 9\n",
      "Evaluating f(y) = siny where y = 0 : 0.0\n",
      "Evaluating f(z) = 5*z where z = f(x) : 45\n"
     ]
    }
   ],
   "source": [
    "from lazydiff.vars import Scalar, Vector\n",
    "from lazydiff import ops\n",
    "import math\n",
    "\n",
    "# creating variable x=3 and y=0\n",
    "x = Scalar(3)\n",
    "y = Scalar(0)\n",
    "\n",
    "# constructing and evaluating f(x) = x^2 and f(y) = sin(y)\n",
    "fx = x**2\n",
    "fy = ops.sin(y)\n",
    "# compound function f(z) = 5*z where z = f(x)\n",
    "fz = 5*fx\n",
    "\n",
    "# accessing the evaluated values\n",
    "# output should print out 9, 0 and 45\n",
    "print(\"Evaluating f(x) = x^2 where x = 3 : {}\".format(fx.val))\n",
    "print(\"Evaluating f(y) = siny where y = 0 : {}\".format(fy.val))\n",
    "print(\"Evaluating f(z) = 5*z where z = f(x) : {}\".format(fz.val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_JlykEZyfhr"
   },
   "source": [
    "### Evaluating Function Gradients\n",
    "\n",
    "As each function is evaluated, the corresponding gradient is also stored in the new output variable. Therefore, each of the output variable contains the gradients from all the previously evaluated functions. This allows us to access the gradient with respect to any of the variables previously computed. For instance, for f(z) from the above example we can compute the gradient with respect to z or x depending on what we pass as input to the gradient method. This also makes it necessary to store the variables of interest somewhere in the program and then use the variable for which we want to evaluate the gradient of as input to the gradient method of the function. \n",
    "\n",
    "Below we show an example of how to access gradients of f(x), f(y), f(z) using `.grad(variable)` method\n",
    "\n",
    "Note that `.grad()` can take multiple variables as input, returning the derivatives with respect to all these variables in the given order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DwGuQaFLyh9T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of f(x) w.r.t x is [6.]\n",
      "Gradient of f(y) w.r.t y is [1.]\n",
      "Gradients of f(z) w.r.t z and x are [ 5. 30.]\n"
     ]
    }
   ],
   "source": [
    "# accessing gradients\n",
    "print(\"Gradient of f(x) w.r.t x is {}\".format(fx.grad(x)))\n",
    "print(\"Gradient of f(y) w.r.t y is {}\".format(fy.grad(y)))\n",
    "# gradient of f(z) w.r.t z is just 5 because d5z/dz = 5\n",
    "# gradient of f(z) w.r.t x is d5x^2/dx = 10x = 30\n",
    "# illustratation of taking two variables as input \n",
    "# and returning the gradient w.r.t. these two variables\n",
    "print(\"Gradients of f(z) w.r.t z and x are {}\".format(fz.grad(fx,x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFRWl-Dl3X3Q"
   },
   "source": [
    "### Working with Vectors\n",
    "\n",
    "`Vector` objects can be instantiated from array of `Scalar` or from multiple `Scalar` arguments. There is no direct support for entering numeric values directly to `Vector` since it is necessary to create and store these `Scalar` for which we used as input for `Vector` to compute the gradient part as mentioned above.\n",
    "\n",
    "Below we show an example of how to initialize `Vector` in two different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bxegV6eX-BOh"
   },
   "outputs": [],
   "source": [
    "# creating an array of Scalars\n",
    "scalar1To5 = [Scalar(i+1) for i in range(5)]\n",
    "# creating 3 different Scalar variables\n",
    "s1,s2,s3,s4,s5= Scalar(1), Scalar(2), Scalar(3), Scalar(4), Scalar(5)\n",
    "\n",
    "# instantiating with array of Scalars\n",
    "x = Vector(scalar1To5)\n",
    "# instantiating with 3 different Scalar variables as input arguments\n",
    "y = Vector(s1,s2,s3,s4,s5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wg2kZaSq-aYD"
   },
   "source": [
    "`Vector` are handled in a similar manner as `Scalar` with similar corresponding operations.\n",
    "\n",
    "Below we show an example evaluating the value and the gradients/Jacobian of $f(z) = 2*z$ where $z = f(x,y)$ and  $f(x,y) = x+y$ where is x,y are the vectors defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImJESK1a-92h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating f(x,y) = x + y : (2, 4, 6, 8, 10)\n",
      "Evaluating f(z) = 2*z : (4, 8, 12, 16, 20)\n",
      "Jacobian of f(x,y) w.r.t. x:\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "Jacobian of f(z) w.r.t. x:\n",
      "[[2. 0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 0.]\n",
      " [0. 0. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# constructing and evaluating f(x,y) and f(z)\n",
    "fxy = x+y\n",
    "fz = 2*fxy\n",
    "\n",
    "# accessing the evaluated values\n",
    "print(\"Evaluating f(x,y) = x + y : {}\".format(fxy.val))\n",
    "print(\"Evaluating f(z) = 2*z : {}\".format(fz.val))\n",
    "\n",
    "# Jacobian of f(x,y) w.r.t x is an identity matrix\n",
    "print(\"Jacobian of f(x,y) w.r.t. x:\\n{}\".format(fxy.grad(x)))\n",
    "# Jacobian of f(z) w.r.t x is 2*identity matrix\n",
    "print(\"Jacobian of f(z) w.r.t. x:\\n{}\".format(fz.grad(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGAsCF0Dy0qH"
   },
   "source": [
    "`Vector` also support `ops` operations just like `Scalar` do!\n",
    "\n",
    "Below we show an example evaluating the value and the gradients/Jacobian of $f(z) = sin(z)$ where $z=f(x)$ and $f(x) = 2x$ where $x=[0,\\frac{\\pi}{2}]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edhngnxrzZG1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating f(x) = 2*x : (0, 3.141592653589793)\n",
      "Evaluating f(z) = sin(z) : (0.0, 1.2246467991473532e-16)\n",
      "Jacobian of f(x) w.r.t. x:\n",
      "[[2. 0.]\n",
      " [0. 2.]]\n",
      "Jacobian of f(z) w.r.t. x:\n",
      "[[ 2.  0.]\n",
      " [ 0. -2.]]\n",
      "Jacobian of f(z) w.r.t. z:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = [Scalar(0), Scalar(math.pi/2)]\n",
    "fx = 2*Vector(x)\n",
    "fz = ops.sin(fx)\n",
    "# 2*x should evaluate to (0,pi)\n",
    "print(\"Evaluating f(x) = 2*x : {}\".format(fx.val))\n",
    "# sin(2x) should evaluate to (0,0)\n",
    "print(\"Evaluating f(z) = sin(z) : {}\".format(fz.val))\n",
    "\n",
    "# Jacobian of f(x) w.r.t. x is 2*identity matrix\n",
    "print(\"Jacobian of f(x) w.r.t. x:\\n{}\".format(fx.grad(x)))\n",
    "# Jacobian of f(z) w.r.t. x has 2 and -2 on the diagonal\n",
    "print(\"Jacobian of f(z) w.r.t. x:\\n{}\".format(fz.grad(x)))\n",
    "# Jacobian of f(z) w.r.t. z has 1 and -1 on the diagonal\n",
    "print(\"Jacobian of f(z) w.r.t. z:\\n{}\".format(fx.grad(fx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OL45Pg0d7shj"
   },
   "source": [
    "### Easy-To-Use Newton's Method with lazzydiff\n",
    "\n",
    "We show how we can use Newton's Method to solve for root of a question using lazzydiff\n",
    "\n",
    "In this case, we explore this using $f(x) = x^2 - 2$, which has a root at $x=\\sqrt2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "P8_gX--H8Ere"
   },
   "outputs": [],
   "source": [
    "def Newton_Method(f, x0, maxiter=1000):\n",
    "    \"\"\" returns the root for the given function\n",
    "        using Newtons Method\n",
    "        \n",
    "        this function supports only Scalar\n",
    "        you can modify this function to support Vector\n",
    "        \n",
    "        ===INPUT===\n",
    "        f: function written in ops or built_in arithmetic functions\n",
    "        x0: initial point\n",
    "        maxiter: maximum number of iterations\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for _ in range(maxiter):\n",
    "        if abs(f(x)).val < 1e-8:\n",
    "            return x\n",
    "        fx = f(x)\n",
    "        newx = x - fx/fx.grad(x)[0]\n",
    "        if (abs(newx-x).val < 1e-8):\n",
    "            return newx\n",
    "        x = newx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3_pFeX68jOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root found by Newton's Method using autodiff is: 1.4142135623746899\n",
      "The difference between this root and the real root 2**0.5 is : 1.5947243525715749e-12\n"
     ]
    }
   ],
   "source": [
    "# the function x^2-2 for which we want to find the root of\n",
    "fx = lambda x: x**2-2\n",
    "ans = Newton_Method(fx, Scalar(1))\n",
    "print(\"The root found by Newton's Method using autodiff is: {}\".format(ans.val))\n",
    "print(\"The difference between this root and the real root 2**0.5 is : {}\".format(ans.val-2**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvK8XHql_9Q7"
   },
   "source": [
    "# Background\n",
    " AD exploits the fact that every function, no matter how complicated, is a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (such as $\\sin$, $\\cos$, $\\exp$, $\\log$, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
    " * **Chain Rule**  \\\\\n",
    "   - AD relies heavily on the chain rule from calculus, which says that $$\\frac{d}{dx} f(g(x)) = \\frac{df}{dg} \\frac{dg}{dx} $$ For a more complex composition $$y = f(g(h(x))) = f(g(h(x_1))) = f(g(x_2)) = f(x_3) = x_4$$ the chain rule gives   \\\\\n",
    "    $$\\frac{dy}{dx} = \\frac{dy}{dx_3} \\frac{dx_3}{dx_2} \\frac{dx_2}{dx_1}\\frac{dx_1}{dx}$$  \\\\\n",
    "    \n",
    " * **Graph structure of calculations**:  \\\\\n",
    " We can think of taking derivatives in AD as building a graph representing evaluation of a function, and using the chain rule to propagate derivative values from input variables to the final function, where each node represents an elementary operation or elementary function.\n",
    " \n",
    " Below is a sample such graph structure resulting from evaluating the function $$f(x_1,x_2) = \\sin(x_1)  + x_1 x_2$$\n",
    " \n",
    " ![alt text](ForwardAccumulationAutomaticDifferentiation.png)\n",
    " [image source](https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png)\n",
    " \n",
    " \n",
    " We see that the input values $x_1,x_2$ and their derivatives $\\dot{w_1}, \\dot{w_2}$ propagate upward and are used in computations for the derivatives $\\dot{w_3}, \\dot{w_4}, \\dot{w_5}$ at each node on the way up to the final function $f(x_1,x_2) = w_5$. \n",
    " \n",
    "<!--  ![alt text](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png =400x200)   \\\\ -->\n",
    " \n",
    "\n",
    "  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BozZIls9KZRv"
   },
   "source": [
    "# Software Organization\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "Our directory structure looks like the following:\n",
    "* lazydiff/\n",
    "     * `__init__.py`\n",
    "     * `variables.py`\n",
    "     * `ops.py`\n",
    "     * tests/\n",
    "       * `__init__.py`\n",
    "       * `test_vars.py`\n",
    "       * `test_ops.py`\n",
    "       * `test_vector.py`\n",
    "       * `test_vector_ops.py`\n",
    "     * extensions/\n",
    "       * `__init__.py`\n",
    "       * `<relevant_classes>.py`\n",
    "* docs/\n",
    " * milestone1.ipynb\n",
    " * milestone2.ipynb\n",
    "* README.md\n",
    "* LICENSE\n",
    "* setup.py\n",
    "* requirements.txt\n",
    "* .travis.yml\n",
    "* setup.cfg\n",
    "\n",
    "\n",
    "All the modules related to autodiff are placed inside the `lazydiff` folder. Files outside this folder are miscellaneous files for setups, Github or test coverages.\n",
    "\n",
    "The key modules included are `variables.py`, which contains our base classes that represent scalar/vector variables and built-in functions for which we may want to compute, and `ops.py`, which contains methods that allow us to evaluate elementary functions such as $\\sin$, $\\cos$, etc.\n",
    "\n",
    "Additional extensions, such as reverse mode, backpropagation etc., will be included in the `extensions` folder. \n",
    "\n",
    "The test suite live in the folder \"tests\" in the directory structure above, and we use Travic CI for continuous integration and Coveralls for test coverage.\n",
    "\n",
    "The packaged library is distributed through Github which can be installed using the pip command and the tutorial is available above in the `How to Install` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdEKrB2oKZxB"
   },
   "source": [
    "# Implementation\n",
    "\n",
    "## Core Data Structures\n",
    "\n",
    "The core of this library relies on the idea of variables, which store a float for the value it represents; a dictionary of previosly computed gradient values given variable reference as key; a list of previously computed variable references.\n",
    "\n",
    "Through this data structure, we can easily find the gradient with respect to any past variable used without recomputing it each time. Please see the demo for an example of this.\n",
    "\n",
    "Vector support is added by creating a vector class that stores a list of corresponding scalars. All the operations are performed on each of these scalars, which should be already implemented.\n",
    "\n",
    "## Core Classes and Important Attributes\n",
    "\n",
    "* In module `vars` :\n",
    "    * We have a class `Scalar` which contains:\n",
    "        * Constructor which takes in a value `val`\n",
    "            * Initializes empty gradient dictionary `grad_val`, which will contain derivatives of this variable with respect to other variables.\n",
    "            * Initializes list representing gradient calculation parents (tuples of parent variables and weight of the gradients of each parent). For instance, if we are evaluating the derivative of $\\sin x$ at $x = 3$, $$\\frac{d}{dx} \\sin x \\Big|_{x = 3}= \\dot{x}|_{x=3} \\cos 3$$ so the list would include the tuple $(x, \\cos 3)$.\n",
    "        * Method ```grad``` which takes in variables, and returns a ndarray containing the derivative with respect to those variables. \n",
    "        * Methods `__add__`, `__mul__` etc for overloading basic arithmetic operations\n",
    "        \n",
    " * We have a class `Vector` which contains:\n",
    "      * Constructor which takes in an array of `Scalar` or multiple `Scalar` arguments\n",
    "       * Initializes a tuple of references to these `Scalar` instances to carry out corresponding operations on them when needed.\n",
    "       * Initializes a tuple of values that these `Scalar` instances currently hold\n",
    "      * Method ```grad``` which takes in variables of interest, and returns a ndarray representing the Jacobian with respect to those variables.\n",
    "      * Methods `__add__`, `__mul__` etc for overloading basic arithmetic operations to support vector operations\n",
    "      \n",
    "## Elementary functions\n",
    "\n",
    "* In module `ops`:\n",
    "    * We have a  method for each elementary function (such as `sin`, `cos`, `exp`, etc.). Each method:\n",
    "        * Take in a `Scalar` or `Vector` object - decorators are used to implement support for `Vector`\n",
    "        * Create a new `Var` object with value updated to reflect execution of elementary function on value of input `Var` object, updated gradient calculation parent list - this compounds functions together\n",
    "        \n",
    "    * We have implemented all the basic arithmetic functions from `math` and `numpy` libraries, by directly calling these functions for evaluating the values. Thus, our `ops` library supports basic elementary functions ranging from `abs` to `asin` etc.\n",
    "    \n",
    "## External Dependencies\n",
    "\n",
    "The implementations rely on `math` and `numpy` for evaluating elementary functions, such as cos, sin, tan. `numpy` is also used to support ndarray.\n",
    "\n",
    "It relies on `numbers` and `collections` to check whether numeric values or containers (for `Vector`) have been entered as input.\n",
    "\n",
    "## Not Implemented Yet\n",
    "\n",
    "We still need to add support for seeding derivatives,  allowing to manually set the derivatives of a variable, i.e. a different value from the default value 1. \n",
    "\n",
    "We are also still considering how to pretty print Scalar or Vector objects in an informative way through `__repr__`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIurSRCWAW6X"
   },
   "source": [
    "# Future\n",
    "\n",
    "## Reverse Mode\n",
    "\n",
    "We are considering to implement reverse mode as one of the extensions. This can be done by storing children instead of parents with the corresponding changes, such as calling operation should append the derivative to the children. The main challenge here is to try reusing the operations `ops` or other methods we already have through perhaps decorators methods, trying to make minimal changes to implement reverse mode, rather than starting it from scratch. \n",
    "\n",
    "## Back Propagation\n",
    "\n",
    "Once reverse mode is implemented, we believe that back propagation can be implemented easily by utilizing the properties of reverse mode. In this case, the main challenge still lies in implementing a reliable and working reverse mode first.\n",
    "\n",
    "## Option for higher-order derivatives (Hessians and beyond)\n",
    "\n",
    "We are thinking of supporting higher-order derivatives, such as Hessians. This can be done by further extending the Vector class with additional methods named as `.Hessian(variables)`. The main challenges of this extension is to figure out how to utilize `.grad(variables)` method to effectively get Hessians or beyond in a recursive manner to prevent reusing duplicate codes. Another challenge is how to do thorough testings of different operations on Hessian and beyond, which can be quite labor intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "v5TJbWlp-sCT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "XqirvYUFggX9",
    "BozZIls9KZRv"
   ],
   "name": "Milestone2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
