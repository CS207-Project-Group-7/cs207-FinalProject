{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from lazydiff import regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LazyDiff\n",
    "\n",
    "## Project Group 7\n",
    "\n",
    "\n",
    "**Team Members**: \n",
    "\n",
    "Joe Davison\n",
    "\n",
    "Raymond Lin\n",
    "\n",
    "Zheng Yang\n",
    "\n",
    "Matteo Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Automatic Differentiation (AD)?\n",
    "\n",
    "- Automatically evaluate the gradient of a function using the chain rule\n",
    "\n",
    "\n",
    "- Numerical approximation is often not accurate enough and can be computationally expensive\n",
    "\n",
    "\n",
    "- Symbolic differentiation tends to lead to inefficient code and faces the difficulty of converting a computer program into a single expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Many uses including backpropagation in neural networks and sampling from complex distributions\n",
    "\n",
    "\n",
    "- Forward and reverse modes of gradient propogation\n",
    "\n",
    "\n",
    "- Uses chain rule to build up gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Forward Mode\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/9b5370f1b8e313d47eb2a2ebac437cf88a7a1d78)\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reverse Mode\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/206c0444486628d70d2617e8eee8a528775e516c)\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LazyDiff\n",
    "\n",
    "- `Var` class wrapping numpy arrays for autodiff scalars and vectors\n",
    "\n",
    "\n",
    "- Custom operations (including elementary functions) through `ops` module\n",
    "\n",
    "\n",
    "- Supports forward and reverse mode (extension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How it works\n",
    "    - `Var` objects store their parents and children, and information on how object relates to its parents and children\n",
    "    - Forward mode requires propogating derivatives to children in topological order\n",
    "    - Reverse mode requires propogating derivaties to parents in topological order\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Setup\n",
    "\n",
    "`pip install lazydiff`\n",
    "\n",
    "_Note: depends on numpy for efficient vectorization_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Manipulate In Terms of Var Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1: Var([1.0, 1.0], seed=1.0)\n"
     ]
    }
   ],
   "source": [
    "from lazydiff.vars import Var\n",
    "\n",
    "# create scalar\n",
    "scalar = Var(1.0)\n",
    "\n",
    "# or vector\n",
    "vector1 = Var([1.0, 1.0])\n",
    "# or using numpy\n",
    "vector2 = Var(np.ones(2))\n",
    "\n",
    "print(\"vector1: {}\".format(vector1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Custom Operations through ops Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fx values are: [2.71828183 2.71828183]\n"
     ]
    }
   ],
   "source": [
    "from lazydiff import ops\n",
    "\n",
    "# conventional\n",
    "fx = ops.exp(vector1)\n",
    "print(\"fx values are: {}\".format(fx.val))\n",
    "\n",
    "# non-conventional\n",
    "fy = ops.sum(vector1)\n",
    "fz = ops.norm(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Getting the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of exp([1,1]):\n",
      "[2.71828183 2.71828183]\n"
     ]
    }
   ],
   "source": [
    "# forward mode\n",
    "vector1.forward()\n",
    "for_grad = fx.grad(vector1)\n",
    "\n",
    "# reverse mode\n",
    "fx.backward()\n",
    "back_grad = fx.grad(vector1)\n",
    "\n",
    "print(\"Gradient of exp([1,1]):\")\n",
    "print(for_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extension - Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://timvieira.github.io/blog/images/backprop-brain-meme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iterative Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X, y, m, b):\n",
    "    # if we don't like m, b\n",
    "    # then we need to extend the column of X with additional 1s\n",
    "    # to factor in +b\n",
    "    loss = Var(0)\n",
    "    for vec, y_i in zip(X,y):\n",
    "        loss = loss + (ops.sum(m*vec)+b-y_i)**2\n",
    "    return loss/len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, loss_function, m, b, lr = 0.1, forward = True):\n",
    "    loss = loss_function(X, y, m, b)\n",
    "    if (forward):\n",
    "        # forward mode\n",
    "        m.forward()\n",
    "        b.forward()\n",
    "    else:\n",
    "        # reverse mode\n",
    "        loss.backward()\n",
    "    # clear cache by reinstantiating\n",
    "    m -= Var(lr*loss.grad(m))\n",
    "    b -= Var(lr*loss.grad(b))\n",
    "    return m, b, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update Iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def iterative_regression(X, y, m, b, loss_function, lr = 0.1,\\\n",
    "        epochs = 100, earlyStop = 0, forward = True):\n",
    "\n",
    "    loss = Var(0)\n",
    "    for ep in range(epochs):\n",
    "        prev = loss\n",
    "        m, b, loss = gradient_descent(X, y, loss_function, m, b, lr, forward)\n",
    "        # check if absolute tolerance meets early stopping condition\n",
    "        if (abs(loss.val - prev.val) < earlyStop):\n",
    "            break\n",
    "    # return coefficient and intercept\n",
    "    return m, b, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](demo.gif)\n",
    "\n",
    "Backup Link: https://youtu.be/n5h-UVG1yYE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Too slow with original implementation\n",
    "\n",
    "![](old_implementation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Issues\n",
    "\n",
    "- Scalar class wrapping each numeric value\n",
    "\n",
    "- Vector class as vector wrapper of Scalar\n",
    "\n",
    "- A lot of unwrapping and wrapping involved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## New Implementation: Wrapper for numpy\n",
    "\n",
    "- One single class, `Var`, wrapping a numpy array\n",
    "\n",
    "- Makes use of numpy array optimization for calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Significant Improvement on Performance\n",
    "\n",
    "- new: time difference noticeable only when incrementing by 1000 features \n",
    "\n",
    "- old: noticeable for each additional feature added\n",
    "\n",
    "Old Implementation            |  New Implementation\n",
    ":-------------------------:|:-------------------------:\n",
    "![](old_implementation.png)  | ![](new_implementation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Extension\n",
    "\n",
    "- Available in `regression` module\n",
    "\n",
    "- Support for Linear, Ridge, Lasso, Elastic Net Regression\n",
    "\n",
    "- Adaptable to Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# implementation for Ridge Regression\n",
    "# define the objective function\n",
    "def ridge_loss(X, y, m, b, C = 1):\n",
    "    loss = Var(0)\n",
    "    for vec, y_i in zip(X,y):\n",
    "        loss = loss + (ops.sum(m*vec)+b-y_i)**2\n",
    "    return loss + C*ops.pow_sum(m,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dim = 2\n",
    "X,y,true_coef = make_regression(n_samples = 100, n_features = dim, n_informative = dim, bias = 10, \\\n",
    "                                coef = True, noise = 1, random_state=1)\n",
    "m_ridge = Var(np.ones(X.shape[1]))\n",
    "b_ridge = Var(0)\n",
    "earlyStop = 0 #1e-8\n",
    "plot = False\n",
    "forward = False\n",
    "epochs = 300\n",
    "# need to use a very small learning rate for Ridge\n",
    "# if not it blows up\n",
    "lr = 0.001\n",
    "m_ridge, b_ridge, loss = regression.iterative_regression(X, y, m_ridge, b_ridge, regression.ridge_loss, lr, epochs,\n",
    "                                                    earlyStop, forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between iterative and sklearn Ridge regression\n",
      "Coefficient: [ 0.00000000e+00 -2.84217094e-14]\n",
      "Intercept: -1.7763568394002505e-15\n"
     ]
    }
   ],
   "source": [
    "clf_l2 = Ridge().fit(X,y)\n",
    "print(\"Difference between iterative and sklearn Ridge regression\")\n",
    "print(\"Coefficient: {}\".format(m_ridge.val - clf_l2.coef_))\n",
    "print(\"Intercept: {}\".format(b_ridge.val - clf_l2.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Let's model $y = x^3-10x^2+3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.linspace(-10,10,20).reshape(-1,1)\n",
    "\n",
    "y = X**3-10*X**2+3\n",
    "X = PolynomialFeatures(3, include_bias = False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "m = Var(np.random.rand(X.shape[1]), seed = 1.0)\n",
    "b = Var(0)\n",
    "earlyStop = 1e-8\n",
    "forward = False\n",
    "m, b, loss = regression.iterative_regression(X, y, m, b, regression.MSE, 0.000001, 10000,\n",
    "                                                    earlyStop, forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](demo_2.gif)\n",
    "\n",
    "Backup Link: https://youtu.be/DF-dh6MBm8Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- AD package wrapping numpy array\n",
    "\n",
    "\n",
    "- Support for both forward and reverse mode\n",
    "\n",
    "\n",
    "- Linear, Lasso, Ridge, Elastic Net, Polynomial Regression support\n",
    "\n",
    "\n",
    "- Easy to generalize to other loss minimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Mention again how using it's much faster with numpy implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Picture Reference\n",
    "\n",
    "- https://wikimedia.org/api/rest_v1/media/math/render/svg/9b5370f1b8e313d47eb2a2ebac437cf88a7a1d78\n",
    "\n",
    "- https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png\n",
    "\n",
    "- https://wikimedia.org/api/rest_v1/media/math/render/svg/206c0444486628d70d2617e8eee8a528775e516c\n",
    "\n",
    "- https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png\n",
    "\n",
    "- https://timvieira.github.io/blog/images/backprop-brain-meme.png"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "cs207",
   "language": "python",
   "name": "cs207"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
